{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network\n",
    "\n",
    "In this notebook, I want to implement a neural network from scratch to understand its architecture. I will implement a simple one layer neural network with two units. \n",
    "\n",
    "Here is how the network will look like (of course, we will be dealing with more inputs!).\n",
    "\n",
    "<img src=\"img/implementing_nnetwork/one_hidden_layer.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "targets = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_standarization(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x_demean = x - np.mean(x)\n",
    "    adjusted_sd = np.maximum(np.std(x), 1.0/np.sqrt(np.prod(x.shape)))\n",
    "    return  x_demean / adjusted_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Standardizing data\n",
    "data = np.apply_along_axis(image_standarization, 1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One hot encoding data\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(targets)\n",
    "targets = encoder.transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating train / test / validation\n",
    "image, image_test, y, y_test = train_test_split(data, targets, test_size=0.2, train_size=0.8)\n",
    "image_train, image_val, y_train, y_val = train_test_split(image, y,test_size = 0.25, train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating useful functions for out network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(input_size, output_size):\n",
    "    \"Generates weights for different units\"\n",
    "    return  np.random.uniform(size=(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_relu(matrix):\n",
    "    \"Applies relu to matrix\"\n",
    "    return np.maximum(0, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_softmax(matrix):\n",
    "    exp = np.exp(matrix) \n",
    "    soft_max = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_apply_softmax(soft_res, n_training):\n",
    "    soft_res[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    return soft_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Our neural network will have two activation functions. First we will generate a matrix multiplication between inputs and first layer and, then, we will apply a ReLU. Then, after the ReLU we will further apply weights and a softmax layers that will produce the probabilities of each of the 10 labels that we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077, 64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los: 6.333350786928165\n",
      "Los: 6.041106890495742\n",
      "Los: 5.774669561004851\n",
      "Los: 5.531172408236043\n",
      "Los: 5.307641849105211\n",
      "Los: 5.101382557714504\n",
      "Los: 4.911126902147293\n",
      "Los: 4.735656890547585\n",
      "Los: 4.573294784712072\n",
      "Los: 4.42286764280514\n",
      "Los: 4.283652122444916\n",
      "Los: 4.154932314978516\n",
      "Los: 4.035845631603817\n",
      "Los: 3.925631980881595\n",
      "Los: 3.8236855085736976\n",
      "Los: 3.729455774348733\n",
      "Los: 3.642242221422941\n",
      "Los: 3.561163833207815\n",
      "Los: 3.4854955981524194\n",
      "Los: 3.414636648024571\n",
      "Los: 3.347899967393138\n",
      "Los: 3.284723763829461\n",
      "Los: 3.224756217131422\n",
      "Los: 3.167557697361559\n",
      "Los: 3.1125994900671987\n",
      "Los: 3.0597425484675003\n",
      "Los: 3.0088030456640302\n",
      "Los: 2.959600896325475\n",
      "Los: 2.911950264952808\n",
      "Los: 2.865556154533466\n",
      "Los: 2.8204112140722093\n",
      "Los: 2.7763357485355664\n",
      "Los: 2.733440566319764\n",
      "Los: 2.6915214321390444\n",
      "Los: 2.6506386769905483\n",
      "Los: 2.6106988296458034\n",
      "Los: 2.5716455204503523\n",
      "Los: 2.5334660278300696\n",
      "Los: 2.4960925123222926\n",
      "Los: 2.4595503801064655\n",
      "Los: 2.423804468175298\n",
      "Los: 2.388855109934094\n",
      "Los: 2.354681638896503\n",
      "Los: 2.3213311700759727\n",
      "Los: 2.288829535187832\n",
      "Los: 2.2571360220268084\n",
      "Los: 2.2262485647647274\n",
      "Los: 2.1961338122227936\n",
      "Los: 2.1667382588923894\n",
      "Los: 2.1380295296792724\n",
      "Los: 2.1100468647730497\n",
      "Los: 2.0827307907400696\n",
      "Los: 2.0560506801043474\n",
      "Los: 2.029999537396729\n",
      "Los: 2.0045797602439723\n",
      "Los: 1.9798194964288442\n",
      "Los: 1.955673260232793\n",
      "Los: 1.9321318471923379\n",
      "Los: 1.9091802873438608\n",
      "Los: 1.8867902046595098\n",
      "Los: 1.8648999861588915\n",
      "Los: 1.8435098402165386\n",
      "Los: 1.8226516750297554\n",
      "Los: 1.8023019885649656\n",
      "Los: 1.7823930818790015\n",
      "Los: 1.7629491696836934\n",
      "Los: 1.743977841402156\n",
      "Los: 1.72545773437629\n",
      "Los: 1.7073602337209588\n",
      "Los: 1.689661258402628\n",
      "Los: 1.672352560849899\n",
      "Los: 1.6554297489739471\n",
      "Los: 1.6388880438843814\n",
      "Los: 1.6226917726204728\n",
      "Los: 1.6068514270311283\n",
      "Los: 1.5913506323613977\n",
      "Los: 1.5761682131022958\n",
      "Los: 1.5613017570640155\n",
      "Los: 1.5467356757680633\n",
      "Los: 1.5324462029620007\n",
      "Los: 1.5184691670451749\n",
      "Los: 1.5047646272396669\n",
      "Los: 1.4913143702748295\n",
      "Los: 1.4781132198465736\n",
      "Los: 1.4651550582328068\n",
      "Los: 1.45245129782565\n",
      "Los: 1.4399675125404447\n",
      "Los: 1.4277026461238556\n",
      "Los: 1.415662518881165\n",
      "Los: 1.4038157371550295\n",
      "Los: 1.3921618296405118\n",
      "Los: 1.3807029823550712\n",
      "Los: 1.3694504527387572\n",
      "Los: 1.3583813245455931\n",
      "Los: 1.347488874457999\n",
      "Los: 1.3367738153608872\n",
      "Los: 1.3262379136719549\n",
      "Los: 1.3158636614478185\n",
      "Los: 1.3056471615532772\n",
      "Los: 1.295579784005717\n"
     ]
    }
   ],
   "source": [
    "n_training = len(image_train)\n",
    "n_classes = 10\n",
    "step_size = 0.0001\n",
    "epochs = 10000\n",
    "\n",
    "#Creating forward pass\n",
    "bias_1 = np.zeros((1,20))\n",
    "bias_2 = np.zeros((1,20))\n",
    "W1 = gen_weights(64, 20) \n",
    "W2 = gen_weights(64, 20)  \n",
    "W3 = gen_weights(20,10)\n",
    "W4 = gen_weights(20,10)\n",
    "\n",
    "for e in range(epochs): \n",
    "    W1 = W1 + bias_1\n",
    "    W2 = W2 + bias_2\n",
    "    h1 = image_train.dot(W1)\n",
    "    h2 = image_train.dot(W2)\n",
    "    relu_1 = apply_relu(h1)\n",
    "    relu_2 = apply_relu(h2)\n",
    "    h3 = relu_1.dot(W3) + relu_2.dot(W4) # Make final class a weighted average of the two units\n",
    "    out_soft = apply_softmax(h3)\n",
    "    \n",
    "    \n",
    "    #Calculate Loss\n",
    "    correct_labels_prob = out_soft[range(n_training), np.argmax(y_train, axis=1)]\n",
    "    loss = np.sum(-np.log(correct_labels_prob)) / n_training\n",
    "    \n",
    "    if e%100==0:\n",
    "        print(\"Los: {}\".format(loss))\n",
    "    \n",
    "    ## Backpropagation\n",
    "    \n",
    "    #Calculating gradient\n",
    "    g_probs = np.copy(out_soft)\n",
    "    g_probs[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    \n",
    "    #Get the average of the gradients\n",
    "    g_probs = g_probs / n_training\n",
    "    \n",
    "    #Calculating gradients for W3 and W4\n",
    "    dW3 = relu_1.T.dot(g_probs)\n",
    "    dW4 = relu_2.T.dot(g_probs)\n",
    "    \n",
    "    #Calculating hidden gradients\n",
    "    dhidden1 = np.dot(g_probs, W3.T)\n",
    "    dhidden2 = np.dot(g_probs, W4.T)\n",
    "    \n",
    "    #Calculating gradients for ReLU's\n",
    "    dhidden1[relu_1 <= 0] = 0\n",
    "    dhidden2[relu_2 <= 0] = 0\n",
    "    #\n",
    "    ##Calculating gradients for initial weights\n",
    "    dW1 = image_train.T.dot(dhidden1)\n",
    "    db1 = np.sum(dhidden1, axis=0, keepdims=True)\n",
    "    #\n",
    "    dW2 = image_train.T.dot(dhidden2)\n",
    "    db2 = np.sum(dhidden2, axis=0, keepdims=True)\n",
    "    #\n",
    "    ##Updating weights\n",
    "    W3 += -step_size*dW3\n",
    "    W4 += -step_size*dW4\n",
    "    #\n",
    "    W1 += -step_size*dW1\n",
    "    bias_1 += -step_size*db1\n",
    "    W2 += -step_size*dW2\n",
    "    bias_2 += -step_size*db2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00045548,  0.00585098,  0.00780963, ...,  0.00614043,\n",
       "         0.00200482,  0.00694802],\n",
       "       [ 0.00433465,  0.00356555,  0.00332302, ...,  0.0026552 ,\n",
       "         0.00188209,  0.00321819],\n",
       "       [ 0.00129413,  0.00867749,  0.0051469 , ...,  0.00723382,\n",
       "         0.00159762,  0.00026414],\n",
       "       ..., \n",
       "       [ 0.00672997,  0.00992511,  0.00703627, ...,  0.00641349,\n",
       "         0.00477615,  0.00601461],\n",
       "       [ 0.00598383,  0.00745225,  0.00247687, ...,  0.00969738,\n",
       "         0.00339087,  0.00784775],\n",
       "       [ 0.0036742 ,  0.00792733,  0.00743393, ...,  0.00898041,\n",
       "         0.00773551,  0.00505815]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_weights(64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([[1,2], [3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

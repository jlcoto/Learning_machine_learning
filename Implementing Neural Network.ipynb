{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network\n",
    "\n",
    "In this notebook, I want to implement a neural network from scratch to understand its architecture. I will implement a simple one layer neural network with two units. \n",
    "\n",
    "Here is how the network will look like (of course, we will be dealing with more inputs!).\n",
    "\n",
    "<img src=\"img/implementing_nnetwork/one_hidden_layer.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "targets = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_standarization(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x_demean = x - np.mean(x)\n",
    "    adjusted_sd = np.maximum(np.std(x), 1.0/np.sqrt(np.prod(x.shape)))\n",
    "    return  x_demean / adjusted_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Standardizing data\n",
    "data = np.apply_along_axis(image_standarization, 1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One hot encoding data\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(targets)\n",
    "targets = encoder.transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating train / test / validation\n",
    "image, image_test, y, y_test = train_test_split(data, targets, test_size=0.2, train_size=0.8)\n",
    "image_train, image_val, y_train, y_val = train_test_split(image, y,test_size = 0.25, train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating useful functions for out network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(input_size, output_size):\n",
    "    \"Generates weights for different units\"\n",
    "    return  np.random.uniform(size=(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_relu(matrix):\n",
    "    \"Applies relu to matrix\"\n",
    "    return np.maximum(0, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_softmax(matrix):\n",
    "    exp = np.exp(matrix) \n",
    "    soft_max = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_apply_softmax(soft_res, n_training):\n",
    "    soft_res[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    return soft_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Our neural network will have two activation functions. First we will generate a matrix multiplication between inputs and first layer and, then, we will apply a ReLU. Then, after the ReLU we will further apply weights and a softmax layers that will produce the probabilities of each of the 10 labels that we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077, 64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los: 4.239157783364263\n",
      "Validation Accuracy: 0.075\n",
      "Los: 3.2034464321275826\n",
      "Validation Accuracy: 0.12777777777777777\n",
      "Los: 2.654334768442389\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Los: 2.2939548171525765\n",
      "Validation Accuracy: 0.21666666666666667\n",
      "Los: 2.0380730812754115\n",
      "Validation Accuracy: 0.28888888888888886\n",
      "Los: 1.8381907358836802\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Los: 1.6728935352344536\n",
      "Validation Accuracy: 0.4\n",
      "Los: 1.5338131071635241\n",
      "Validation Accuracy: 0.46944444444444444\n",
      "Los: 1.4142932849557925\n",
      "Validation Accuracy: 0.5027777777777778\n",
      "Los: 1.3111038108836983\n",
      "Validation Accuracy: 0.5444444444444444\n",
      "Los: 1.221484669043867\n",
      "Validation Accuracy: 0.5777777777777777\n",
      "Los: 1.1429271816205424\n",
      "Validation Accuracy: 0.6083333333333333\n",
      "Los: 1.0738750012203897\n",
      "Validation Accuracy: 0.6416666666666667\n",
      "Los: 1.0127699563590753\n",
      "Validation Accuracy: 0.6638888888888889\n",
      "Los: 0.9584886840384986\n",
      "Validation Accuracy: 0.6916666666666667\n",
      "Los: 0.9098117906058133\n",
      "Validation Accuracy: 0.7111111111111111\n",
      "Los: 0.8659749874701929\n",
      "Validation Accuracy: 0.7277777777777777\n",
      "Los: 0.8263485246560134\n",
      "Validation Accuracy: 0.7361111111111112\n",
      "Los: 0.7903277683627533\n",
      "Validation Accuracy: 0.7527777777777778\n",
      "Los: 0.7575799031016918\n",
      "Validation Accuracy: 0.7694444444444445\n",
      "Los: 0.7274940329781995\n",
      "Validation Accuracy: 0.7805555555555556\n",
      "Los: 0.6998139623084678\n",
      "Validation Accuracy: 0.8055555555555556\n",
      "Los: 0.6743515082620933\n",
      "Validation Accuracy: 0.8111111111111111\n",
      "Los: 0.6508568228196534\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.6291056749697194\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.6088808777890324\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.5899979509670875\n",
      "Validation Accuracy: 0.8388888888888889\n",
      "Los: 0.5723699744242854\n",
      "Validation Accuracy: 0.8416666666666667\n",
      "Los: 0.555823780274818\n",
      "Validation Accuracy: 0.85\n",
      "Los: 0.5401880758857945\n",
      "Validation Accuracy: 0.85\n",
      "Los: 0.5254765806259523\n",
      "Validation Accuracy: 0.8527777777777777\n",
      "Los: 0.5116673688887754\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.49862570051659816\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.48630677908775677\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.4746260294132306\n",
      "Validation Accuracy: 0.8583333333333333\n",
      "Los: 0.4635699679327616\n",
      "Validation Accuracy: 0.8638888888888889\n",
      "Los: 0.45308639880354734\n",
      "Validation Accuracy: 0.8694444444444445\n",
      "Los: 0.443111935979175\n",
      "Validation Accuracy: 0.8722222222222222\n",
      "Los: 0.433613774416782\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4245600990898703\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4159092221312846\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4076587137095661\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.39974908612189014\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.3922027023396793\n",
      "Validation Accuracy: 0.8805555555555555\n",
      "Los: 0.38496277791431094\n",
      "Validation Accuracy: 0.8805555555555555\n",
      "Los: 0.3780010464213751\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.37131867830248605\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.3649174092380214\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.35873903733923335\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.35279415703901035\n",
      "Validation Accuracy: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "n_training = len(image_train)\n",
    "n_classes = 10\n",
    "step_size = 0.0001\n",
    "epochs = 50000\n",
    "\n",
    "#Creating forward pass\n",
    "bias_1 = np.zeros((1,20))\n",
    "bias_2 = np.zeros((1,20))\n",
    "W1 = gen_weights(64, 20) \n",
    "W2 = gen_weights(64, 20)  \n",
    "W3 = gen_weights(20,10)\n",
    "W4 = gen_weights(20,10)\n",
    "\n",
    "for e in range(epochs): \n",
    "    W1 = W1 + bias_1\n",
    "    W2 = W2 + bias_2\n",
    "    h1 = image_train.dot(W1)\n",
    "    h2 = image_train.dot(W2)\n",
    "    relu_1 = apply_relu(h1)\n",
    "    relu_2 = apply_relu(h2)\n",
    "    h3 = relu_1.dot(W3) + relu_2.dot(W4) # Make final class a weighted average of the two units\n",
    "    out_soft = apply_softmax(h3)\n",
    "    \n",
    "    \n",
    "    #Calculate Loss\n",
    "    correct_labels_prob = out_soft[range(n_training), np.argmax(y_train, axis=1)]\n",
    "    loss = np.sum(-np.log(correct_labels_prob)) / n_training\n",
    "    \n",
    "    if e%1000==0:\n",
    "        print(\"Los: {}\".format(loss))\n",
    "    \n",
    "    ## Backpropagation\n",
    "    \n",
    "    #Calculating gradient\n",
    "    g_probs = np.copy(out_soft)\n",
    "    g_probs[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    \n",
    "    #Get the average of the gradients\n",
    "    g_probs = g_probs / n_training\n",
    "    \n",
    "    #Calculating gradients for W3 and W4\n",
    "    dW3 = relu_1.T.dot(g_probs)\n",
    "    dW4 = relu_2.T.dot(g_probs)\n",
    "    \n",
    "    #Calculating hidden gradients\n",
    "    dhidden1 = np.dot(g_probs, W3.T)\n",
    "    dhidden2 = np.dot(g_probs, W4.T)\n",
    "    \n",
    "    #Calculating gradients for ReLU's\n",
    "    dhidden1[relu_1 <= 0] = 0\n",
    "    dhidden2[relu_2 <= 0] = 0\n",
    "    #\n",
    "    ##Calculating gradients for initial weights\n",
    "    dW1 = image_train.T.dot(dhidden1)\n",
    "    db1 = np.sum(dhidden1, axis=0, keepdims=True)\n",
    "    #\n",
    "    dW2 = image_train.T.dot(dhidden2)\n",
    "    db2 = np.sum(dhidden2, axis=0, keepdims=True)\n",
    "    #\n",
    "    ##Updating weights\n",
    "    W3 += -step_size*dW3\n",
    "    W4 += -step_size*dW4\n",
    "    #\n",
    "    W1 += -step_size*dW1\n",
    "    bias_1 += -step_size*db1\n",
    "    W2 += -step_size*dW2\n",
    "    bias_2 += -step_size*db2\n",
    "\n",
    "    #Test Validation Accuracy   \n",
    "    h1_val = image_val.dot(W1)\n",
    "    h2_val = image_val.dot(W2)\n",
    "    relu_1_val = apply_relu(h1_val)\n",
    "    relu_2_val = apply_relu(h2_val)\n",
    "    h3_val = relu_1_val.dot(W3) + relu_2_val.dot(W4) # Make final class a weighted average of the two units\n",
    "    preds_score = apply_softmax(h3_val)\n",
    "    pred_ones = np.zeros_like(preds_score) #generates zero matrix to populate them with ones to then make comparisons\n",
    "    pred_ones[np.arange(len(preds_score)), preds_score.argmax(1)] = 1\n",
    "    accuracy = np.mean(np.equal(pred_ones, y_val).all(axis=1))\n",
    "\n",
    "    if e%1000==0:\n",
    "        print(\"Validation Accuracy: {}\".format(accuracy))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "#Getting testing accuracy\n",
    "#Testing predictions on testing data set\n",
    "#Test Validation Accuracy   \n",
    "h1_test = image_test.dot(W1)\n",
    "h2_test = image_test.dot(W2)\n",
    "relu_1_test = apply_relu(h1_test)\n",
    "relu_2_test = apply_relu(h2_test)\n",
    "h3_test = relu_1_test.dot(W3) + relu_2_test.dot(W4) # Make final class a weighted average of the two units\n",
    "preds_score = apply_softmax(h3_test)\n",
    "pred_ones = np.zeros_like(preds_score) #generates zero matrix to populate them with ones to then make comparisons\n",
    "pred_ones[np.arange(len(preds_score)), preds_score.argmax(1)] = 1\n",
    "accuracy = np.mean(np.equal(pred_ones, y_test).all(axis=1))\n",
    "print(\"Testing Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

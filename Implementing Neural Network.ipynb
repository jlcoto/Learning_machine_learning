{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network\n",
    "\n",
    "In this notebook, I want to implement a neural network from scratch to understand its architecture. I will implement a simple one layer neural network with two units. \n",
    "\n",
    "Here is how the network will look like (of course, we will be dealing with more inputs!).\n",
    "\n",
    "<img src=\"img/implementing_nnetwork/one_hidden_layer.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "targets = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_standarization(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    x_demean = x - np.mean(x)\n",
    "    adjusted_sd = np.maximum(np.std(x), 1.0/np.sqrt(np.prod(x.shape)))\n",
    "    return  x_demean / adjusted_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Standardizing data\n",
    "data = np.apply_along_axis(image_standarization, 1, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One hot encoding data\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(targets)\n",
    "targets = encoder.transform(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating train / test / validation\n",
    "image, image_test, y, y_test = train_test_split(data, targets, test_size=0.2, train_size=0.8)\n",
    "image_train, image_val, y_train, y_val = train_test_split(image, y,test_size = 0.25, train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating useful functions for out network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(input_size, output_size):\n",
    "    \"Generates weights for different units\"\n",
    "    return  np.random.uniform(size=(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_relu(matrix):\n",
    "    \"Applies relu to matrix\"\n",
    "    return np.maximum(0, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_softmax(matrix):\n",
    "    exp = np.exp(matrix) \n",
    "    soft_max = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    return soft_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def d_apply_softmax(soft_res, n_training):\n",
    "    soft_res[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    return soft_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Our neural network will have two activation functions. First we will generate a matrix multiplication between inputs and first layer and, then, we will apply a ReLU. Then, after the ReLU we will further apply weights and a softmax layers that will produce the probabilities of each of the 10 labels that we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1077, 64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los: 4.239157783364263\n",
      "Validation Accuracy: 0.075\n",
      "Los: 3.2034464321275826\n",
      "Validation Accuracy: 0.12777777777777777\n",
      "Los: 2.654334768442389\n",
      "Validation Accuracy: 0.16666666666666666\n",
      "Los: 2.2939548171525765\n",
      "Validation Accuracy: 0.21666666666666667\n",
      "Los: 2.0380730812754115\n",
      "Validation Accuracy: 0.28888888888888886\n",
      "Los: 1.8381907358836802\n",
      "Validation Accuracy: 0.3333333333333333\n",
      "Los: 1.6728935352344536\n",
      "Validation Accuracy: 0.4\n",
      "Los: 1.5338131071635241\n",
      "Validation Accuracy: 0.46944444444444444\n",
      "Los: 1.4142932849557925\n",
      "Validation Accuracy: 0.5027777777777778\n",
      "Los: 1.3111038108836983\n",
      "Validation Accuracy: 0.5444444444444444\n",
      "Los: 1.221484669043867\n",
      "Validation Accuracy: 0.5777777777777777\n",
      "Los: 1.1429271816205424\n",
      "Validation Accuracy: 0.6083333333333333\n",
      "Los: 1.0738750012203897\n",
      "Validation Accuracy: 0.6416666666666667\n",
      "Los: 1.0127699563590753\n",
      "Validation Accuracy: 0.6638888888888889\n",
      "Los: 0.9584886840384986\n",
      "Validation Accuracy: 0.6916666666666667\n",
      "Los: 0.9098117906058133\n",
      "Validation Accuracy: 0.7111111111111111\n",
      "Los: 0.8659749874701929\n",
      "Validation Accuracy: 0.7277777777777777\n",
      "Los: 0.8263485246560134\n",
      "Validation Accuracy: 0.7361111111111112\n",
      "Los: 0.7903277683627533\n",
      "Validation Accuracy: 0.7527777777777778\n",
      "Los: 0.7575799031016918\n",
      "Validation Accuracy: 0.7694444444444445\n",
      "Los: 0.7274940329781995\n",
      "Validation Accuracy: 0.7805555555555556\n",
      "Los: 0.6998139623084678\n",
      "Validation Accuracy: 0.8055555555555556\n",
      "Los: 0.6743515082620933\n",
      "Validation Accuracy: 0.8111111111111111\n",
      "Los: 0.6508568228196534\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.6291056749697194\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.6088808777890324\n",
      "Validation Accuracy: 0.8222222222222222\n",
      "Los: 0.5899979509670875\n",
      "Validation Accuracy: 0.8388888888888889\n",
      "Los: 0.5723699744242854\n",
      "Validation Accuracy: 0.8416666666666667\n",
      "Los: 0.555823780274818\n",
      "Validation Accuracy: 0.85\n",
      "Los: 0.5401880758857945\n",
      "Validation Accuracy: 0.85\n",
      "Los: 0.5254765806259523\n",
      "Validation Accuracy: 0.8527777777777777\n",
      "Los: 0.5116673688887754\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.49862570051659816\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.48630677908775677\n",
      "Validation Accuracy: 0.8555555555555555\n",
      "Los: 0.4746260294132306\n",
      "Validation Accuracy: 0.8583333333333333\n",
      "Los: 0.4635699679327616\n",
      "Validation Accuracy: 0.8638888888888889\n",
      "Los: 0.45308639880354734\n",
      "Validation Accuracy: 0.8694444444444445\n",
      "Los: 0.443111935979175\n",
      "Validation Accuracy: 0.8722222222222222\n",
      "Los: 0.433613774416782\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4245600990898703\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4159092221312846\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.4076587137095661\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.39974908612189014\n",
      "Validation Accuracy: 0.875\n",
      "Los: 0.3922027023396793\n",
      "Validation Accuracy: 0.8805555555555555\n",
      "Los: 0.38496277791431094\n",
      "Validation Accuracy: 0.8805555555555555\n",
      "Los: 0.3780010464213751\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.37131867830248605\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.3649174092380214\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.35873903733923335\n",
      "Validation Accuracy: 0.8833333333333333\n",
      "Los: 0.35279415703901035\n",
      "Validation Accuracy: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "n_training = len(image_train)\n",
    "n_classes = 10\n",
    "step_size = 0.0001\n",
    "epochs = 50000\n",
    "\n",
    "#Creating forward pass\n",
    "bias_1 = np.zeros((1,20))\n",
    "bias_2 = np.zeros((1,20))\n",
    "W1 = gen_weights(64, 20) \n",
    "W2 = gen_weights(64, 20)  \n",
    "W3 = gen_weights(20,10)\n",
    "W4 = gen_weights(20,10)\n",
    "\n",
    "for e in range(epochs): \n",
    "    W1 = W1 + bias_1\n",
    "    W2 = W2 + bias_2\n",
    "    h1 = image_train.dot(W1)\n",
    "    h2 = image_train.dot(W2)\n",
    "    relu_1 = apply_relu(h1)\n",
    "    relu_2 = apply_relu(h2)\n",
    "    h3 = relu_1.dot(W3) + relu_2.dot(W4) # Make final class a weighted average of the two units\n",
    "    out_soft = apply_softmax(h3)\n",
    "    \n",
    "    \n",
    "    #Calculate Loss\n",
    "    correct_labels_prob = out_soft[range(n_training), np.argmax(y_train, axis=1)]\n",
    "    loss = np.sum(-np.log(correct_labels_prob)) / n_training\n",
    "    \n",
    "    if e%1000==0:\n",
    "        print(\"Los: {}\".format(loss))\n",
    "    \n",
    "    ## Backpropagation\n",
    "    \n",
    "    #Calculating gradient\n",
    "    g_probs = np.copy(out_soft)\n",
    "    g_probs[range(n_training), np.argmax(y_train, axis=1)] -= 1\n",
    "    \n",
    "    #Get the average of the gradients\n",
    "    g_probs = g_probs / n_training\n",
    "    \n",
    "    #Calculating gradients for W3 and W4\n",
    "    dW3 = relu_1.T.dot(g_probs)\n",
    "    dW4 = relu_2.T.dot(g_probs)\n",
    "    \n",
    "    #Calculating hidden gradients\n",
    "    dhidden1 = np.dot(g_probs, W3.T)\n",
    "    dhidden2 = np.dot(g_probs, W4.T)\n",
    "    \n",
    "    #Calculating gradients for ReLU's\n",
    "    dhidden1[relu_1 <= 0] = 0\n",
    "    dhidden2[relu_2 <= 0] = 0\n",
    "    #\n",
    "    ##Calculating gradients for initial weights\n",
    "    dW1 = image_train.T.dot(dhidden1)\n",
    "    db1 = np.sum(dhidden1, axis=0, keepdims=True)\n",
    "    #\n",
    "    dW2 = image_train.T.dot(dhidden2)\n",
    "    db2 = np.sum(dhidden2, axis=0, keepdims=True)\n",
    "    #\n",
    "    ##Updating weights\n",
    "    W3 += -step_size*dW3\n",
    "    W4 += -step_size*dW4\n",
    "    #\n",
    "    W1 += -step_size*dW1\n",
    "    bias_1 += -step_size*db1\n",
    "    W2 += -step_size*dW2\n",
    "    bias_2 += -step_size*db2\n",
    "\n",
    "    #Test Validation Accuracy   \n",
    "    h1_val = image_val.dot(W1)\n",
    "    h2_val = image_val.dot(W2)\n",
    "    relu_1_val = apply_relu(h1_val)\n",
    "    relu_2_val = apply_relu(h2_val)\n",
    "    h3_val = relu_1_val.dot(W3) + relu_2_val.dot(W4) # Make final class a weighted average of the two units\n",
    "    preds_score = apply_softmax(h3_val)\n",
    "    pred_ones = np.zeros_like(preds_score) #generates zero matrix to populate them with ones to then make comparisons\n",
    "    pred_ones[np.arange(len(preds_score)), preds_score.argmax(1)] = 1\n",
    "    accuracy = np.mean(np.equal(pred_ones, y_val).all(axis=1))\n",
    "\n",
    "    if e%1000==0:\n",
    "        print(\"Validation Accuracy: {}\".format(accuracy))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "#Getting testing accuracy\n",
    "#Testing predictions on testing data set\n",
    "#Test Validation Accuracy   \n",
    "h1_test = image_test.dot(W1)\n",
    "h2_test = image_test.dot(W2)\n",
    "relu_1_test = apply_relu(h1_test)\n",
    "relu_2_test = apply_relu(h2_test)\n",
    "h3_test = relu_1_test.dot(W3) + relu_2_test.dot(W4) # Make final class a weighted average of the two units\n",
    "preds_score = apply_softmax(h3_test)\n",
    "pred_ones = np.zeros_like(preds_score) #generates zero matrix to populate them with ones to then make comparisons\n",
    "pred_ones[np.arange(len(preds_score)), preds_score.argmax(1)] = 1\n",
    "accuracy = np.mean(np.equal(pred_ones, y_test).all(axis=1))\n",
    "print(\"Testing Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with one specific image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116b54128>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116a88c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.argmax(y_test[10]))\n",
    "plt.imshow(image_test[10].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_test = image_test[10].dot(W1)\n",
    "h2_test = image_test[10].dot(W2)\n",
    "relu_1_test = apply_relu(h1_test)\n",
    "relu_2_test = apply_relu(h2_test)\n",
    "h3_test = relu_1_test.dot(W3) + relu_2_test.dot(W4) # Make final class a weighted average of the two units\n",
    "exp = np.exp(h3_test) \n",
    "soft_max = exp / np.sum(exp, keepdims=True)\n",
    "np.argmax(soft_max) ## Correctly guesses the number -> Assigns more probability to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
